# <to_left>Hao-xuan (Horace) Wang</to_left>  <description_nleft><to_right>[ +86 18622468042](tel://008618622468042)</to_right><br> <to_right>[billweasley20092@gmail.com](billweasley20092@gmail.com) </to_right><br><to_right><b>Github: </b> [https://github.com/billweasley](https://github.com/billweasley)</to_right><br><to_right><b>Linkedin: </b> [https://www.linkedin.com/in/horace-haoxuan-wang](https://www.linkedin.com/in/horace-haoxuan-wang)</to_right><br><to_right><b>Personal Website: </b> [http://shellcottage.me](http://shellcottage.me)</to_right></description_nleft>    

Work Experience
--------
- <datetime>2022.03 - Now </datetime> <head_><head_title> Machine Learning Engineer, ASR and Language Tech </head_title> @ Zoom </head_>
<description><small>
Tech stack: Torch/Pytorch, ONNX, Kubernates, Huggingface
<ul>
<ul><li> Used a closed-source LLM for ASR error correction post-processing, extracting the N-best list from the Conformer-Transducer model, and wrote prompts combined with a biasing word list to send to GPT-4 for named entity correction. Offline experiments on a medical dataset showed that the <b>Rare word WER decreased from 37.8 to 17.5</b>.</li>
<li> Wrote meta-prompts, using open-source LLM models (e.g., Mistral MoE 8x7B) to generate hundreds of dialogue scene prompts, and combined them with different reading formats of numerical sequences generated by the LLM model. These were used to create dialogue scenarios with numerical texts, which were handed over to colleagues to generate audio using internal/Microsoft TTS services, resulting in a test set and part of the training data. Fine-tuned the production model with the training data resulted in a slight improvement on another internal digits dataset (<b>Absolute digit WER decreased by about 0.4%</b>).</li>
<li> (Ongoing) Use SLAM-LLM to experiment with ASR/LLM integration for speech + text modality SFT, aiming to improve the consistency of ASR decoding results.</li>
<li> Trained a speech recognition and text punctuation model, building a LAS-S2S Danish model from scratch. The initial word error rate (WER) on the test dataset was about <b>8%</b>, further reduced by phoneme data augmentation, <b>outperforming MS Teams' results</b>. The initial overall F1 score for case and punctuation was about <b>70%</b>.</li>
<li> Independently implemented <b>Whisper</b> inference support, optimization, and performance evaluation (WER, RTF/latency/throughput), using an in-house VAD (voice activity detection) model and open-source WhisperX. Achieved higher throughput compared to OpenAI's implementation and lower WER on most test sets.</li>
<li> Implemented, tested, maintained, and evaluated key functions of the inference system, including <b>decoding</b>, <b>timestamp generation</b>, and <b>post-processing</b> code.</li>
<li> Implemented multi-head attention (MHA) based time alignment on LAS/seq2seq models to provide good word-level timestamps, meeting the business needs for multilingual transcription. [Filed a US patent for this]</li>
<li> Collaborated with downstream web and infrastructure teams to streamline the offline transcription system architecture and deployment, supporting <b>35+</b> single-language models across <b>10+</b> different regions for <b>5+</b> different downstream services.</li>
<li> Evaluated the performance of the speech recognition system, seeking the most suitable parameters to reduce real-time factors or improve system throughput.</li></ul>
</ul>
</small></description>

- <datetime>2019.08 - 2021.12 </datetime> <head_><head_title> Data Scientist and Software Engineer </head_title> @ Barclays </head_>
<description><small>
Tech stack: Spark / PySpark, Amazon Deep Java Library (DJL), Tensorflow / Keras, Pandas, Jupyter, Pretrained Transformers / Likelihood Ratio  
<ul>
<li>
Company address matching and entity matching without internal GPU and labeled data available. Solve using an active learning method. Start from constructing some small datasets only with external data and training an XGBoost tree, then label samples in the boundary and fine-turn BERT models in an iterative way. Finish the inference on 6 million internal pair-wised samples with this model on a CPU cluster, using a DJL based pipeline built from scratch on my own. It achieved a very satisfying result of <b>94% F1 score on a noisy testing dataset from 89% where we started</b>. The model does inference offline on our Spark cluster in a distributed way. For 6 million pair-wised samples, the running time is under 1 hour (on a cluster with 80 CPUs).
</li>
<li>
Predict the aggregated user's transaction activity (volume and value) using the historical mean and Informer model, a variant of Transformer for time-series modeling. Following that, a counterfactual was constructed to provide an evaluation of how much finance loss that the bank suffers from system downtime and to find out the critical period for the system reliability.
</li>
<li>
Maintain the Spark cluster for the team, and build up pipelines for distributed inference by combining DJL / PySpark UDF with models.Collaborated with one of my colleagues, we created a team-wised package to start a Spark session within 4 lines of codes, which significantly reduces the overhead of using Spark for colleagues who are not with a distributed computing background.
</li>
</ul>
</small></description>

Education
--------
<ul style="list-style-type: none;">
<li><head_><datetime>2018 - 2019</datetime> MSc Web Science and Big Data Analytics  @&nbsp;<b>University College London, </b>&nbsp;Distinction</head_></li>
<li><head_><datetime>2016 - 2018</datetime> BSc Internet Computing  @&nbsp;<b>University of Liverpool *, </b>&nbsp; First class</head_></li>
<li><head_><datetime>2014 - 2016</datetime> BSc Information and Computing Science  @&nbsp;<b>Xi'an Jiaotong-Liverpool University * </b>&nbsp;</head_>
<li><description><small><b>*Note: </b>2+2 pathway routine (first 2 years in Suzhou, China and final 2 years in Liverpool, UK), dual degree.</small></description></li>
</li>
</ul>

Personal Project
--------
- <head_><datetime>2024.06 - </datetime> <head_title>Fine-tuning and evaluation of medical record data on Large Language Models (LLMs) </head_title> </head_>
<description>
<small>
（Ongoing）Using hundreds of thousands of Chinese medical case data, fine-tune different LLM foundation models (Llama3-instruct, Llama3 Chinese-chat, Qwen2) on tasks such as department classification, medical record summarization, and discharge certification. On the domain-specific test dataset it achieved significant improvements in scenarios like consultation summary/discharge summary (BLEU 0% ~ 30% -> 49% ~ 55%, ROUGE-L 20% ~ 30% -> 60% ~ 64%), and scenario like the department classification for multi-class accuracy (Accuracy 0% ~ 36% -> 69% ~ 71%). We plan to open source the data in the future.
</small>
</description>

Technical Article
--------
- <head_><head_title>"Accelerating Deep Learning on the JVM with Apache Spark and NVIDIA GPUs" </head_title> </head_>
<description><small>
Author: Haoxuan Wang, Qin Lan [AWS], Carol McDonald [Nvidia];  Link: https://www.infoq.com/articles/deep-learning-apache-spark-nvidia-gpu/?itm_source=articles_about_ai-ml-data-eng&itm_medium=link&itm_campaign=ai-ml-data-eng
</small>
</description>


Early Stage Project
--------
- <head_><datetime>2019.06 - 2019.09</datetime> <head_title>Project Internship (Master Degree Thesis) </head_title>  @ Astroscreen </head_>
<description>
<small>
Social media posting language source identification (tweets and gabs) project.
Finished a crawler for collecting language (posts) data from Gab.com, pre-processed data using Regular Expression, built models for classifying the source of these data by fine-turning BERT and XLNet,
visualized results using t-SNE, did "leave-one-hashtag-out" cross-validation, and evaluated models using some common matrics (Accuracy, F1 score, Confusion Matrix, Matthews Correlation Coefficient). After fine-turning, XLNet shows a 86% F1 score on hashtag-balanced test dataset, reducing from 97% on random-balanced test dataset. The results show the protential for doing source checking using a model and also indicate the importance for avoiding data leakage.
</small>
</description>